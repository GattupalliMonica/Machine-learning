# -*- coding: utf-8 -*-
"""monica assignment -2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/142LIxB8oQx3lKZkSTR60NYO815w_3Tb3
"""

#importing the libraries 
import pandas as pd
import numpy as np
import statistics as st
import sklearn.model_selection
import time
import sklearn.metrics
import scipy.stats as sc
from sklearn.metrics import cluster
from time import process_time  
from sklearn.model_selection import cross_val_score,train_test_split,StratifiedKFold
from sklearn.metrics import confusion_matrix, f1_score, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier 
from sklearn.preprocessing import KBinsDiscretizer

#intial variables
i=1
acc_rfc=0
acc_knn=0
acc_dt=0
cri_diff=[]
accuracy=[[],[],[]]
ti=[[],[],[]]
f_m=[[],[],[]]

# Reading the data in csv format
X=pd.read_csv("Spambase.csv",usecols=range(54))
Y=pd.read_csv("Spambase.csv",usecols=[57]).values

# Discretizer with 12 bins and follows the uniform strategy.
dis = KBinsDiscretizer(n_bins=12, encode='ordinal', strategy='uniform')
dis.fit(X)
Xt=dis.transform(X)
y=Y.ravel()

#crossvalidation of data with number ofsplits as 10.
#CSK means : Crossvalidation with stratified Kfold.
CSK = StratifiedKFold(n_splits=10, shuffle=False,random_state=None)

#measuring accuracy
def accuracy_measu(x,y):#contingency  and instance spaces are used as parameters
    c=0 
    for i in range(2):
       c=c+x[i][i]
    return(c/y)
        

# performing Friedman test
print("\n\n Performing Friedman test for the selected algorithms")

def fd_test(a,n):
    print("\t DecisionTree \t K-nearest neighbour \t\t RandomForest")
    fdt=[]
    a_rank=[] #Ranking
    arank=2
    c_val=7.8 #critical value mentioned in the textbook
    sum=0
    if(n==1):
        i=0
        while i<10:
            fdt.append(sc.rankdata([a[0][i],a[1][i],a[2][i]]))
            print("\t",round(a[0][i],2),"(",fdt[i][0],")","\t\t\t",round(a[1][i],2),"(",fdt[i][1],")","\t\t\t",round(a[2][i],2),"(",fdt[i][2],")")
            i=i+1
    else:
        i=0
        dummy=[]
        while i<10:
            dummy=pd.Series([a[0][i],a[1][i],a[2][i]])
            fdt.append(dummy.rank(ascending=False))
            print("\t",round(a[0][i],2),"(",fdt[i][0],")","\t\t\t",round(a[1][i],2),"(",fdt[i][1],")","\t\t\t",round(a[2][i],2),"(",fdt[i][2],")")
            dummy=[]
            i=i+1
       
    for i in range(0,3):
        avg=0       
        for j in range(10):
            avg+=fdt[j][i]
        a_rank.append(avg/10)
    print("---------------------------------------------------------------------------------------")
    print("avgrank ",abs(a_rank[0]),"\t\t\t\t",abs(a_rank[1]),"\t\t\t\t",abs(a_rank[2]))
    
    
   
    sum=0 
    for i in range(10):
      des_a= sum+ ((a[0][i]-a_rank[0])**2)
      knn_a= sum+ ((a[1][i]-a_rank[1])**2)
      rdf_a= sum+((a[2][i]-a_rank[2])**2)
    #des_a= round(a[0].mean(),2)
    des_m=des_a/10
    knn_m=knn_a/10
    rdf_m=rdf_a/10
    F=10*(des_m+knn_m+rdf_m)
    print("Friedman statistics",F)
    
    if(F<c_val):     
	    print('\n\nIn this case accepted the NUll hypothesis')
    else: print('\n\nIn this case rejected the null hypothesis , so we are further moving to nemeyi test'); nemeyi(a)

# Nemeyi test definition
def nemeyi(nem): 
    fdt=[]
    a_rank=[]
    #critical difference
    c_val=(2.343)*(0.2**0.5)
    i = 0
    while i<10:
        fdt.append(sc.rankdata([nem[0][i],nem[1][i],nem[2][i]])) 
        i=i+1 
    for i in range(0,3):
        avg=0
        j=0
        
        while j<10:

            avg+=fdt[j][i]
            j=j+1
        a_rank.append(avg/10)
    cri_diff.append(a_rank[0]-a_rank[1])
    cri_diff.append(a_rank[2]-a_rank[1])
    cri_diff.append(a_rank[0]-a_rank[2])
    if(c_val<cri_diff[0]):
        print("\nCritical difference is less than the difference b/w algorithms dt and knn ")
    if(c_val<cri_diff[1]):
        print("\nCritical difference is less than the difference b/w algorithms knn and rf")
    if(c_val<cri_diff[2]):
        print("\nCritical difference is less than the difference b/w algorithms rf and dt")
    

#Classifiers training and testing for performance
for train, test in CSK.split(Xt,y):
    Xt_train, Xt_test = Xt[train], Xt[test]
    y_train, y_test = y[train], y[test]
    knn=KNeighborsClassifier(n_neighbors=2,p=2,metric='euclidean').fit(Xt_train, y_train)
    rfc=RandomForestClassifier(n_estimators=100).fit(Xt_train,y_train)
    dt= DecisionTreeClassifier(criterion = "gini",random_state = 100,max_depth=3, min_samples_leaf=5).fit(Xt_train,y_train)
# Calculation start and end time of the 3 algorithms   
#Decision tree classifier 
    startdt=time.process_time()
    dt_pre=dt.predict(Xt_test)
    dt_result= cluster.contingency_matrix(y_test, dt_pre, sparse=False)
    stopdt=time.process_time()
    acc_dt+=accuracy_measu(dt_result,460)     
    result_dt=((2*dt_result[0][0])/((2*dt_result[0][0])+dt_result[1][0]+dt_result[0][1]))

#KNN classifier
    startknn = time.process_time()
    knn_pre=knn.predict(Xt_test)
    knn_result= cluster.contingency_matrix(y_test, knn_pre,sparse=False)
    stopknn=time.process_time()
    acc_knn+=accuracy_measu(knn_result,460)
    result_knn=((2*knn_result[0][0])/((2*knn_result[0][0])+knn_result[1][0]+knn_result[0][1]))
    
#random forest classifier
    startrfc=time.process_time()
    rfc_pre=rfc.predict(Xt_test)
    rfc_result= cluster.contingency_matrix(y_test, rfc_pre,sparse=False)
    stoprfc=time.process_time()
    acc_rfc+=accuracy_measu(rfc_result,460)   
    result_rfc=((2*rfc_result[0][0])/((2*rfc_result[0][0])+rfc_result[1][0]+rfc_result[0][1]))  


#accuracy of the 3 algorithms
    accuracy[0].append(accuracy_measu(dt_result,460))
    accuracy[1].append(accuracy_measu(knn_result,460))
    accuracy[2].append(accuracy_measu(rfc_result,460))

# time measurement of the 3 algorithms
    ti[0].append(stopdt-startdt)
    ti[1].append(stopknn-startknn)
    ti[2].append(stoprfc-startrfc)

# fmeasures of 3 classifiers
    f_m[0].append(result_dt)
    f_m[1].append(result_knn)
    f_m[2].append(result_rfc)
    
# Accuracy measurements of the classifiers
print("\n\nAccuarcy table")
print("\t DecisionTree \t K-nearest neighbour \t\t Randomforest")
k=0
while k<10:
    print("\t",round(accuracy[0][k],3),"\t\t\t\t",round(accuracy[1][k],3),"\t\t\t\t",round(accuracy[2][k],3))
    k=k+1
print("---------------------------------------------------------------------------------------")
print("a_acc ",round(st.mean(accuracy[0]),3),"\t\t\t\t",round(st.mean(accuracy[1]),3),"\t\t\t\t",round(st.mean(accuracy[2]),3))
print("st_dev ",round(st.stdev(accuracy[0]),3),"\t\t\t\t",round(st.stdev(accuracy[1]),3),"\t\t\t\t",round(st.stdev(accuracy[2]),3))

fd_test(accuracy,0)



# F-meaures of the classifiers
print("\n\nF-measure table")
print("\t DecisionTree \t K-nearest neighbour \t\t RandomForest")
k=0
while k<10:
    print("\t",round(f_m[0][k],3),"\t\t\t\t",round(f_m[1][k],3),"\t\t\t\t",round(f_m[2][k],3))
    k=k+1
print("---------------------------------------------------------------------------------------")
print("a___f ",round(st.mean(f_m[0]),3),"\t\t\t\t",round(st.mean(f_m[1]),3),"\t\t\t\t",round(st.mean(f_m[2]),3))
print("st_dev ",round(st.stdev(f_m[0]),3),"\t\t\t\t",round(st.stdev(f_m[1]),3),"\t\t\t\t",round(st.stdev(f_m[2]),3))
fd_test(f_m,0)


# Time measurements of the classifiers
print("\n\nTime measurement table")
print("\t Decision Tree \t\t K-nearest neighbour \t\t\t\t RandomForest")
k=0
while k <10:
    print("\t",round(ti[0][k],3),"\t\t\t\t",round(ti[1][k],3),"\t\t\t\t",round(ti[2][k],3))
    k=k+1
print("---------------------------------------------------------------------------------------")
print("avgtime ",round(st.mean(ti[0]),3),"\t\t\t\t",round(st.mean(ti[1]),3),"\t\t\t\t",round(st.mean(ti[2]),3))
print("st_dev ",round(st.stdev(ti[0]),3),"\t\t\t\t",round(st.stdev(ti[1]),3),"\t\t\t\t",round(st.stdev(ti[2]),3))
fd_test(ti,1)

